$ start-all.sh
$ cd IST3134
$ pig -x local -d error

-- Load the traffic data from CSV file
grunt> traffic = LOAD 'traffic.csv' USING PigStorage(',') AS (
    ID: int,
    SPEED: float,
    TRAVEL_TIME: int,
    STATUS: int,
    DATA_AS_OF: chararray,
    LINK_ID: chararray,
    LINK_POINTS: chararray,
    OWNER: chararray,
    TRANSCOM_ID: int,
    BOROUGH: chararray,
    LINK_NAME: chararray
);

-- Filter and project the relevant columns for anomaly detection
grunt> speed_travel_time = FOREACH traffic GENERATE SPEED, TRAVEL_TIME;

-- Calculate mean for SPEED
grunt> speed_mean = FOREACH (GROUP speed_travel_time ALL) GENERATE AVG(speed_travel_time.SPEED) AS avg_speed;

-- Calculate standard deviation for SPEED
grunt> speed_std = FOREACH (GROUP speed_travel_time ALL) {
    squared_diff = FOREACH speed_travel_time GENERATE (SPEED - speed_mean.avg_speed) * (SPEED - speed_mean.avg_speed) AS diff_squared;
    GENERATE SQRT(AVG(squared_diff.diff_squared)) AS std_speed;
}

-- Calculate mean for TRAVEL_TIME
grunt> travel_time_mean = FOREACH (GROUP speed_travel_time ALL) GENERATE AVG(speed_travel_time.TRAVEL_TIME) AS avg_travel_time;

-- Calculate standard deviation for TRAVEL_TIME
grunt> travel_time_std = FOREACH (GROUP speed_travel_time ALL) {
    squared_diff = FOREACH speed_travel_time GENERATE (TRAVEL_TIME - travel_time_mean.avg_travel_time) * (TRAVEL_TIME - travel_time_mean.avg_travel_time) AS diff_squared;
    GENERATE SQRT(AVG(squared_diff.diff_squared)) AS std_travel_time;
}

-- Cross join the stats relations
grunt> stats_cross = CROSS speed_mean, speed_std, travel_time_mean, travel_time_std;

-- Generate the final stats relation
grunt> stats = FOREACH stats_cross GENERATE
    speed_mean::avg_speed AS avg_speed,
    speed_std::std_speed AS std_speed,
    travel_time_mean::avg_travel_time AS avg_travel_time,
    travel_time_std::std_travel_time AS std_travel_time;

-- Standardize the data (subtract mean and divide by standard deviation)
grunt> speed_travel_time_std = FOREACH speed_travel_time GENERATE
    (SPEED - stats.avg_speed) / stats.std_speed AS SPEED_STD,
    (TRAVEL_TIME - stats.avg_travel_time) / stats.std_travel_time AS TRAVEL_TIME_STD;

-- Convert the data to a bag
grunt> speed_travel_time_bag = FOREACH speed_travel_time_std GENERATE TOTUPLE(SPEED_STD, TRAVEL_TIME_STD) AS data;

----duplicate another new session
----run the Python code in the Hadoop cluster
$ pip install sklearn
$ start_time=$(date +%s)
$ nano lof.py
$ python3 lof.py
$ end_time=$(date +%s)
$ execution_time=$((end_time - start_time))
$ echo "Execution time: ${execution_time} seconds"

$ cd IST3134/preprocessed_data.csv
$ cat part-m-00000 | less
$ cat part-m-00001 | less

-----in pig
-- Load the results from the output file
anomalies = LOAD '/home/hadoop/IST3134/outputpig.csv' USING PigStorage(',') AS (data:tuple, lof_score:double);

-- Join the anomalies back with the original traffic data
anomalies_with_traffic = JOIN anomalies BY (SPEED, TRAVEL_TIME), traffic BY (SPEED, TRAVEL_TIME);

-- Store the anomalies in a separate file
STORE anomalies_with_traffic INTO 'anomalies1.csv' USING PigStorage(',');

-- Display the anomalies
DUMP anomalies_with_traffic;

-back to the terminal
$ cd IST3134/anomalies1.csv
$ cat part-r-00000 | less
$ cat part-r-00001 | less
