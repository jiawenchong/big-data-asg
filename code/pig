$ start-all.sh
$ cd IST3134
$ pig -x local -d error

traffic = LOAD '/home/hadoop/IST3134/traffic.csv' USING PigStorage(',') AS (
    ID: int,
    SPEED: float,
    TRAVEL_TIME: int,
    STATUS: int,
    DATA_AS_OF: chararray,
    LINK_ID: chararray,
    OWNER: chararray,
    TRANSCOM_ID: int,
    BOROUGH: chararray, 
    LINK_NAME: chararray
);

traffic_display = FOREACH traffic GENERATE CONCAT('ID: ', (chararray)ID), 
                                        CONCAT('SPEED: ', (chararray)SPEED),
                                        CONCAT('TRAVEL_TIME: ', (chararray)TRAVEL_TIME),
                                        CONCAT('STATUS: ', (chararray)STATUS),
                                        CONCAT('DATA_AS_OF: ', DATA_AS_OF),
                                        CONCAT('LINK_ID: ', LINK_ID),
                                        CONCAT('OWNER: ', OWNER),
                                        CONCAT('TRANSCOM_ID: ', (chararray)TRANSCOM_ID),
                                        CONCAT('BOROUGH: ', BOROUGH),
                                        CONCAT('LINK_NAME: ', LINK_NAME);

-- Limit the display to the first 10 rows
limited_display = LIMIT traffic_display 5;

-- Display the first 10 rows of the 'limited_display' relation
DUMP limited_display;

-- Filter and project the relevant columns for anomaly detection
grunt> speed_travel_time = FOREACH traffic GENERATE SPEED, TRAVEL_TIME;

-- Calculate mean for SPEED
grunt> speed_mean = FOREACH (GROUP speed_travel_time ALL) GENERATE AVG(speed_travel_time.SPEED) AS avg_speed;

-- Calculate standard deviation for SPEED
grunt> speed_std = FOREACH (GROUP speed_travel_time ALL) {
    squared_diff = FOREACH speed_travel_time GENERATE (SPEED - speed_mean.avg_speed) * (SPEED - speed_mean.avg_speed) AS diff_squared;
    GENERATE SQRT(AVG(squared_diff.diff_squared)) AS std_speed;
}

-- Calculate mean for TRAVEL_TIME
grunt> travel_time_mean = FOREACH (GROUP speed_travel_time ALL) GENERATE AVG(speed_travel_time.TRAVEL_TIME) AS avg_travel_time;

-- Calculate standard deviation for TRAVEL_TIME
grunt> travel_time_std = FOREACH (GROUP speed_travel_time ALL) {
    squared_diff = FOREACH speed_travel_time GENERATE (TRAVEL_TIME - travel_time_mean.avg_travel_time) * (TRAVEL_TIME - travel_time_mean.avg_travel_time) AS diff_squared;
    GENERATE SQRT(AVG(squared_diff.diff_squared)) AS std_travel_time;
}

-- Cross join the stats relations
grunt> stats_cross = CROSS speed_mean, speed_std, travel_time_mean, travel_time_std;

-- Generate the final stats relation
grunt> stats = FOREACH stats_cross GENERATE
    speed_mean::avg_speed AS avg_speed,
    speed_std::std_speed AS std_speed,
    travel_time_mean::avg_travel_time AS avg_travel_time,
    travel_time_std::std_travel_time AS std_travel_time;

-- Standardize the data (subtract mean and divide by standard deviation)
grunt> speed_travel_time_std = FOREACH speed_travel_time GENERATE
    (SPEED - stats.avg_speed) / stats.std_speed AS SPEED_STD,
    (TRAVEL_TIME - stats.avg_travel_time) / stats.std_travel_time AS TRAVEL_TIME_STD;

STORE speed_travel_time_std INTO 'preprocessed_data.csv' USING PigStorage(',');

----duplicate another new session
----run the Python code in the Hadoop cluster
$ pip install scikit-learn
$ nano lof.py
$ start_time=$(date +%s)
$ python3 lof.py
$ end_time=$(date +%s)
$ execution_time=$((end_time - start_time))
$ echo "Execution time: ${execution_time} seconds"

$ cd IST3134/preprocessed_data.csv
$ cat part-m-00000 | less
$ cat part-m-00001 | less

-----in pig
-- Load the results from the output file
anomalies = LOAD '/home/hadoop/IST3134/outputpig.csv' USING PigStorage(',') AS (data:tuple, lof_score:double);

-- Join the anomalies back with the original traffic data
anomalies_with_traffic = JOIN anomalies BY (SPEED, TRAVEL_TIME), traffic BY (SPEED, TRAVEL_TIME);

-- Store the anomalies in a separate file
STORE anomalies_with_traffic INTO 'anomalies1.csv' USING PigStorage(',');

-- Display the anomalies
DUMP anomalies_with_traffic;

-back to the terminal
$ cd IST3134/anomalies1.csv
$ cat part-r-00000 | less
$ cat part-r-00001 | less

-----
-- Load the results from the output file containing filtered anomalies
anomalies = LOAD '/home/hadoop/IST3134/outputpig.csv' USING PigStorage(',') AS (SPEED: float, TRAVEL_TIME: int, LOF_SCORE: double);



-- Join the anomalies back with the original traffic data based on SPEED and TRAVEL_TIME columns
anomalies_with_traffic = JOIN anomalies BY (SPEED, TRAVEL_TIME), traffic BY (SPEED, TRAVEL_TIME);

-- Project the desired columns in the output relation (including the BOROUGH column)
anomalies_with_borough = FOREACH anomalies_with_traffic GENERATE anomalies::SPEED AS SPEED,
                                                           anomalies::TRAVEL_TIME AS TRAVEL_TIME,
                                                           anomalies::LOF_SCORE AS LOF_SCORE,
                                                           traffic::BOROUGH AS BOROUGH;

-- Store the anomalies with borough information in a separate file
STORE anomalies_with_borough INTO 'anomalies_with_borough.csv' USING PigStorage(',');

-- Display the limited anomalies with borough information (only 50 rows)
limited_anomalies = LIMIT anomalies_with_borough 50;
DUMP limited_anomalies;
