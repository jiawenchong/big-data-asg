$ start-all.sh
$ cd IST3134
$ pig -x local -d error

-- Load the traffic data from CSV file
grunt> traffic = LOAD 'traffic.csv' USING PigStorage(',') AS (
    ID: int,
    SPEED: float,
    TRAVEL_TIME: int,
    STATUS: int,
    DATA_AS_OF: chararray,
    LINK_ID: chararray,
    LINK_POINTS: chararray,
    OWNER: chararray,
    TRANSCOM_ID: int,
    BOROUGH: chararray,
    LINK_NAME: chararray
);

-- Filter and project the relevant columns for anomaly detection
grunt> speed_travel_time = FOREACH traffic GENERATE SPEED, TRAVEL_TIME;

-- Calculate mean for SPEED
grunt> speed_mean = FOREACH (GROUP speed_travel_time ALL) GENERATE AVG(speed_travel_time.SPEED) AS avg_speed;

-- Calculate standard deviation for SPEED
grunt> speed_std = FOREACH (GROUP speed_travel_time ALL) {
    squared_diff = FOREACH speed_travel_time GENERATE (SPEED - speed_mean.avg_speed) * (SPEED - speed_mean.avg_speed) AS diff_squared;
    GENERATE SQRT(AVG(squared_diff.diff_squared)) AS std_speed;
}

-- Calculate mean for TRAVEL_TIME
grunt> travel_time_mean = FOREACH (GROUP speed_travel_time ALL) GENERATE AVG(speed_travel_time.TRAVEL_TIME) AS avg_travel_time;

-- Calculate standard deviation for TRAVEL_TIME
grunt> travel_time_std = FOREACH (GROUP speed_travel_time ALL) {
    squared_diff = FOREACH speed_travel_time GENERATE (TRAVEL_TIME - travel_time_mean.avg_travel_time) * (TRAVEL_TIME - travel_time_mean.avg_travel_time) AS diff_squared;
    GENERATE SQRT(AVG(squared_diff.diff_squared)) AS std_travel_time;
}

-- Cross join the stats relations
grunt> stats_cross = CROSS speed_mean, speed_std, travel_time_mean, travel_time_std;

-- Generate the final stats relation
grunt> stats = FOREACH stats_cross GENERATE
    speed_mean::avg_speed AS avg_speed,
    speed_std::std_speed AS std_speed,
    travel_time_mean::avg_travel_time AS avg_travel_time,
    travel_time_std::std_travel_time AS std_travel_time;

-- Standardize the data (subtract mean and divide by standard deviation)
grunt> speed_travel_time_std = FOREACH speed_travel_time GENERATE
    (SPEED - stats.avg_speed) / stats.std_speed AS SPEED_STD,
    (TRAVEL_TIME - stats.avg_travel_time) / stats.std_travel_time AS TRAVEL_TIME_STD;

-- Convert the data to a bag
grunt> speed_travel_time_bag = FOREACH speed_travel_time_std GENERATE TOTUPLE(SPEED_STD, TRAVEL_TIME_STD) AS data;

----duplicate another new session
----run the Python code in the Hadoop cluster
$ pip install sklearn
$ start_time=$(date +%s)
$ nano lof.py
$ python3 lof.py
import csv
from sklearn.neighbors import LocalOutlierFactor

# Specify the input file path
input_file = '/home/hadoop/IST3134/preprocessed_data.csv/part-m-00000'

# Specify the output file path
output_file = '/home/hadoop/IST3134/outputpig.csv'

data = []

with open(input_file, 'r') as csvfile:
    reader = csv.reader(csvfile)
    for record in reader:
        speed = float(record[0].replace('(', '').replace(')', ''))
        travel_time = float(record[1].replace('(', '').replace(')', ''))
        data.append([speed, travel_time])

# Run the LOF algorithm on the data
lof = LocalOutlierFactor(n_neighbors=5)
lof_scores = lof.fit_predict(data)

# Generate the LOF scores
results = []
for i, score in enumerate(lof_scores):
    results.append([data[i][0], data[i][1], score])

# Write results to CSV file
with open(output_file, 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerows(results)

$ end_time=$(date +%s)
$ execution_time=$((end_time - start_time))
$ echo "Execution time: ${execution_time} seconds"


-----in pig
-- Load the results from the output file
anomalies = LOAD '/home/hadoop/IST3134/pigoutput1.csv' USING PigStorage(',') AS (data:tuple, lof_score:double);

-- Join the anomalies back with the original traffic data
anomalies_with_traffic = JOIN anomalies BY data, traffic BY (SPEED, TRAVEL_TIME);

-- Store the anomalies in a separate file
STORE anomalies_with_traffic INTO 'anomalies1.csv' USING PigStorage(',');

-- Display the anomalies
DUMP anomalies_with_traffic;
