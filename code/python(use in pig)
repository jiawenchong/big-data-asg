import pandas as pd
import numpy as np
from pyod.models.lof import LOF
from sklearn.utils import shuffle

# Specify the input file path
input_file = '/home/hadoop/IST3134/preprocessed_data.csv/part-m-00000'

# Load the data and filter out rows with NA values in 'SPEED', 'TRAVEL_TIME', or 'BOROUGH' columns
data_df = pd.read_csv(input_file, na_values=["", "NA", "NaN"])
data_df = data_df.dropna(subset=["SPEED", "TRAVEL_TIME"])

# Take a random sample
sample_size = 200000
data_sample = shuffle(data_df, random_state=123)[:sample_size]

# Run the LOF algorithm on the data sample
data_for_lof = data_sample[["SPEED", "TRAVEL_TIME"]]
lof_model = LOF(n_neighbors=5)
lof_model.fit(data_for_lof)
lof_scores = lof_model.decision_scores_

# Add the LOF scores to the DataFrame
data_sample["lof_score"] = lof_scores

# Specify the output file path
output_file = '/home/hadoop/IST3134/anomalies1.csv'

# Write results to CSV file
data_sample.to_csv(output_file, index=False)

# Perform data manipulations and save the final result to CSV file
data_sample["BOROUGH"] = data_sample["BOROUGH"].str.upper()
data_sample["has_na"] = np.where(data_sample["lof_score"].isnull(), 1, 0)

result = data_sample.groupby("BOROUGH").agg(
    count_leq1=pd.NamedAgg(column="lof_score", aggfunc=lambda x: sum(x <= 1)),
    count_gt1=pd.NamedAgg(column="lof_score", aggfunc=lambda x: sum(x > 1)),
    count_na=pd.NamedAgg(column="has_na", aggfunc="sum")
).reset_index()

# Print the result (optional)
print(result)

# Write the final result to CSV file
output_result_file = "/home/hadoop/IST3134/outpig.csv"
result.to_csv(output_result_file, index=False)

print("Result saved to:", output_result_file)
